[
  
  {
    "title": "代码解读：Event-based Stereo Visual Odometry",
    "url": "/posts/ESVO/",
    "categories": "TecDoc, Code Understanding",
    "tags": "Cpp, Event-based Vision, ESVO",
    "date": "2023-10-17 23:33:33 +0800",
    





    
    "snippet": "Event-based Stereo Visual Odometry 提出了一种基于双目事件相机的视觉里程计。在 Github上开源了代码，项目主要由 Cpp 实现，运行环境为 ROS, 本文的重点并不是如何运行代码，所以忽略了 ROS 中节点以及 launch file.  挖个坑以后填上，本文旨在对其中的功能实现的部分关键代码进行解读。本文之后的部分将 Event-based Stere...",
    "content": "Event-based Stereo Visual Odometry 提出了一种基于双目事件相机的视觉里程计。在 Github上开源了代码，项目主要由 Cpp 实现，运行环境为 ROS, 本文的重点并不是如何运行代码，所以忽略了 ROS 中节点以及 launch file.  挖个坑以后填上，本文旨在对其中的功能实现的部分关键代码进行解读。本文之后的部分将 Event-based Stereo Visual Odometry 简写为ESVO.为了提高计算效率，达到实时性的要求，ESVO 采用了多线程的方法，将计算任务分配给多个线程，同时进行计算。Cpp 多线程的操作有待深入学习。现在看不懂  TODO: Cpp MultithreadingSystem FlowchartESVO 的 flowchat 如下图所示，主要由三大部分组成，分别是 Event Preprocessing, Mapping, Tracking.  Event Preprocessing  对输入的原始事件流进行滤波，生成 Time-Surface Maps.  Mapping  接收Time-Surface, 进行双目匹配，得到深度，生成 Local Map 与 Point Cloud.  Tracking  对 Local Map 进行跟踪，生成相机的 Pose(6Dof), 将变换矩阵返回给 Mapping 模块ESVO System Flowchart1Code UnderstandingESVO 代码组成如下图所示，主要由三个模块组成，分别是 esvo_time_surface, esvo_Mapping, esvo_Tracking.  TimeSurface  esvo_time_surface 中的 TimeSurface 实现了 Event Preprocessing 部分，对输入的原始事件流进行滤波，生成 Time-Surface Maps.  esvo_Mapping  esvo_core 中的 esvo_Mapping 实现了 Mapping 部分，接收 Time-Surface，得到深度，生成 Local Map 与 Point Cloud.  esvo_Tracking  esvo_core 中的 esvo_Tracking 实现了 Tracking 部分，对 Local Map 进行跟踪，生成相机的 Pose(6Dof), 将变换矩阵返回给 Mapping 模块  esvo_MVStereo  esvo_core 中的 esvo_MVStereo 实现了 ESVO mapper 部分，以及其他的一些 event-based mapping methods23. 作为 multi-view stereo (MVS) pipeline, 该模块需要先验的 pose 作为输入ESVO Treemap from Scientific Toolworks Understand v6.4Time-SurfaceTime-Surface 是一种将事件流转换为类似于图像帧的数据结构的方法，使得事件流的信息可以用传统的计算机视觉算法处理。Time-Surface 的核心思想是将事件流的时间信息转换为空间信息，将事件流的时间戳 $t_i$ 转换为像素的灰度，这样就可以将事件流转换为图像帧。本博客在之前已经对 Time-Surface 做了详细的说明，你可以点击这里查看：事件相机图像重构：浅谈Time-SurfaceESVO 中的 Time-Surface 实现在 esvo_time_surface 中，TimeSurface_node.cpp在 ROS 中作为节点运行，TimeSurface.cpp 实现了 Time-Surface 的核心算法以及滤波和矫正等功能。由于水平有限所以在这里只分析单线程的 void TimeSurface::createTimeSurfaceAtTime(const ros::Time&amp; external_sync_time)的实现，多线程的 void TimeSurface::createTimeSurfaceAtTime_hyperthread(const ros::Time&amp; external_sync_time)有待后续解读，但基本原理是相同的。esvo_time_surface 文件架构如下图所示，TimeSurface_node.cpp 在 ROS 中启动节点，TimeSurface.cpp 实现了主要算法：esvo_time_surface architecture from Scientific Toolworks Understand v6.4代码主要为两个循环，遍历所有坐标，若 getMostRecentEventBeforeT返回值为真，则将 most_recent_event_at_coordXY_before_T的时间戳转换为灰度值，赋值给 time_surface_map:  TODO:流程图.{: .prompt-info }void TimeSurface::createTimeSurfaceAtTime(const ros::Time&amp; external_sync_time){  // create exponential-decayed Time Surface map.  // Loop through all coordinates  for(int y=0; y&lt;sensor_size_.height; ++y)  {    for(int x=0; x&lt;sensor_size_.width; ++x)    {    }// loop x  }// loop y  // polarity  // median blur  // Publish event image}Init something在 TimeSurface::createTimeSurfaceAtTime()中：  // create exponential-decayed Time Surface map.  const double decay_sec = decay_ms_ / 1000.0;  cv::Mat time_surface_map;  time_surface_map = cv::Mat::zeros(sensor_size_, CV_64F);getMostRecentEventBeforeT函数 getMostRecentEventBeforeT的功能是找到(x,y)处距离现在的时间 t 最近的事件，是较为关键的函数该函数的作用是找到 [x, y] 处的一系列事件流中最新的一个事件：  // 找到距离时间 t 最近的事件  bool getMostRecentEventBeforeT(    const size_t x,    const size_t y,    const ros::Time&amp; t,    dvs_msgs::Event* ev)  {    if(!insideImage(x, y))      return false;    // using EventQueue = std::deque&lt;dvs_msgs::Event&gt;;    // 双向开口的deque，可以从头尾两端进行插入和删除操作    EventQueue&amp; eq = getEventQueue(x, y); // 访问向量中第 x + width_ * y 个元素    if(eq.empty())      return false; // 如果该像素点没有事件，返回false    // 从后往前遍历deque，找到第一个时间戳小于t的事件    // 即找到距离时间 t 最近的事件    for(auto it = eq.rbegin(); it != eq.rend(); ++it)    {      const dvs_msgs::Event&amp; e = *it;      if(e.ts &lt; t)      {        *ev = *it;        return true;      }    }    return false;  }Create Time-Surface遍历所有坐标，若 getMostRecentEventBeforeT返回值为真，则将 most_recent_event_at_coordXY_before_T的时间戳转换为灰度值，赋值给 time_surface_map:  // Loop through all coordinates  for(int y=0; y&lt;sensor_size_.height; ++y)  {    for(int x=0; x&lt;sensor_size_.width; ++x)    {      dvs_msgs::Event most_recent_event_at_coordXY_before_T;      if(pEventQueueMat_-&gt;getMostRecentEventBeforeT(x, y, external_sync_time, &amp;most_recent_event_at_coordXY_before_T))      {        // Time-Surface Implementation                 }      }// loop x  }// loop yTime-Surface 有两种实现方式，Backward version 和 Forward version.Backward versionBackward version 在事件坐标系下生成的，它直接在事件坐标系下生成 Time-Surface, 不需要相机的内参和外参信息。从当前时间 t 开始，向前计算，时间戳越近，灰度值越大，即越亮。公式定义如下所示：\\[\\mathcal{T}(\\mathbf{x},t)\\doteq\\exp\\left(-\\frac{t-t_{\\mathsf{last}}(\\mathbf{x})}\\eta\\right)\\]其中：$t_last$ 是$(x,y)$处最近（最新）的事件的时间戳，即 most_recent_event_at_coordXY_before_T.ts$t$ 是当前系统的时间戳，即 external_sync_time          const double dt = (external_sync_time - most_recent_stamp_at_coordXY).toSec();          double polarity = (most_recent_event_at_coordXY_before_T.polarity) ? 1.0 : -1.0;          double expVal = std::exp(-dt / decay_sec);          if(!ignore_polarity_)            expVal *= polarity;          // Backward version          if(time_surface_mode_ == BACKWARD)            time_surface_map.at&lt;double&gt;(y,x) = expVal;Forward versionForward version 在相机坐标系下生成的，将事件投影到相机的图像平面上，然后在图像平面上生成 Time-Surface. 需要相机的内参和外参信息。为了加快计算，使用了 look-up table 的方法，将事件投影到相机的图像平面上，然后在图像平面上生成 Time-Surface.  // load look-up table  for (size_t i = 0; i &lt; sensor_size.height * sensor_size.width; i++)  {    precomputed_rectified_points_.col(i) = Eigen::Matrix&lt;double, 2, 1&gt;(      RectCoordinates(i).x, RectCoordinates(i).y);  }          // Forward version          if(time_surface_mode_ == FORWARD &amp;&amp; bCamInfoAvailable_)          {            Eigen::Matrix&lt;double, 2, 1&gt; uv_rect = precomputed_rectified_points_.block&lt;2, 1&gt;(0, y * sensor_size_.width + x);            size_t u_i, v_i;            if(uv_rect(0) &gt;= 0 &amp;&amp; uv_rect(1) &gt;= 0)            {              u_i = std::floor(uv_rect(0)); // x coordinate, 下取整              v_i = std::floor(uv_rect(1)); // y coordinate, 下取整              if(u_i + 1 &lt; sensor_size_.width &amp;&amp; v_i + 1 &lt; sensor_size_.height) //  防越界              {                double fu = uv_rect(0) - u_i;                double fv = uv_rect(1) - v_i;                double fu1 = 1.0 - fu;                double fv1 = 1.0 - fv;                time_surface_map.at&lt;double&gt;(v_i, u_i) += fu1 * fv1 * expVal;                time_surface_map.at&lt;double&gt;(v_i, u_i + 1) += fu * fv1 * expVal;                time_surface_map.at&lt;double&gt;(v_i + 1, u_i) += fu1 * fv * expVal;                time_surface_map.at&lt;double&gt;(v_i + 1, u_i + 1) += fu * fv * expVal;                if(time_surface_map.at&lt;double&gt;(v_i, u_i) &gt; 1)                  time_surface_map.at&lt;double&gt;(v_i, u_i) = 1;                if(time_surface_map.at&lt;double&gt;(v_i, u_i + 1) &gt; 1)                  time_surface_map.at&lt;double&gt;(v_i, u_i + 1) = 1;                if(time_surface_map.at&lt;double&gt;(v_i + 1, u_i) &gt; 1)                  time_surface_map.at&lt;double&gt;(v_i + 1, u_i) = 1;                if(time_surface_map.at&lt;double&gt;(v_i + 1, u_i + 1) &gt; 1)                  time_surface_map.at&lt;double&gt;(v_i + 1, u_i + 1) = 1;              }MappingMapping 部分代码在 esvo_core 中实现， Mapping 的主要功能是接收双目 Time-Surface 以及变换矩阵，计算出带有深度信息的 mapping。ESVO 提出了一种基于非线性优化衡量事件流时空一致性的目标函数的 mapping 方法。首先对左右目进行 Block Match，得到左右目的视差图，然后将视差图作为 Nonlinear Optimization and Fusion 的初值，通过非线性优化得到深度图。在 esvo_core\\src\\core\\esvo_Mapping.cpp 中：函数调用关系如下所示：graph LRA --&gt; F[esvo_Mapping::InitializationAtTime]F --&gt;|Yes| B[esvo_Mapping::MappingAtTime]A[esvo_Mapping::MappingLoop]B[esvo_Mapping::MappingAtTime]subgraph Nonlinear Optimization and FusionE[DepthProblemSolver::solve]endsubgraph Block MatchingC[esvo_core::core::EventBM::createMatchProblem]D[esvo_core::core::EventBM::match_all_HyperThread]endB --&gt; EB --&gt; CB --&gt; DMapping 初始化 Event Batch Matcher，ebm_ 是一个类对象，它的类型是 EventBM. 构造函数的参数是 ebm_(camSysPtr_, NUM_THREAD_MAPPING, tools::param(pnh_, \"SmoothTimeSurface\", false)),.  // initialize Event Batch Matcher  ebm_.resetParameters(BM_patch_size_X_, BM_patch_size_Y_, minDisparity, maxDisparity,                       BM_step_, BM_ZNCC_Threshold_, BM_bUpDownConfiguration_);Mapiing 初始化 dpSolver_, 在 esvo_core\\include\\esvo_core\\esvo_Mapping.h 中定义为 DepthProblemSolver dpSolver_;DepthProblemSolver 定义在 esvo_core\\include\\esvo_core\\core\\DepthProblemSolver.h    dpSolver_(camSysPtr_, dpConfigPtr_, NUMERICAL, NUM_THREAD_MAPPING),Mapping 中启动了 MappingThread  // stereo mapping detached thread  std::thread MappingThread(&amp;esvo_Mapping::MappingLoop, this,                            std::move(mapping_thread_promise_), std::move(reset_future_));  MappingThread.detach();MappingLoop 中调用了 MappingAtTimeTS_obs_ 用于传递 Time-Surface, TS_obs_ 的类型是 std::pair&lt;ros::Time, TimeSurface&gt;，其中 TimeSurface 是一个结构体，它包含了一个指向 cv::Mat 的智能指针，以及一个时间戳。  std::pair&lt;ros::Time, TimeSurface&gt; TS_obs_;      // Do mapping      if(ESVO_System_Status_ == \"WORKING\")        MappingAtTime(TS_obs_.first);InitializationAtTime  TODOMappingAtTime初始化新的 DepthFrame, 并将其存储在类成员变量 depthFramePtr_ 中。  TicToc tt_mapping;  double t_overall_count = 0;  /************************************************/  /************ set the new DepthFrame ************/  /************************************************/  DepthFrame::Ptr depthFramePtr_new = std::make_shared&lt;DepthFrame&gt;(    camSysPtr_-&gt;cam_left_ptr_-&gt;height_, camSysPtr_-&gt;cam_left_ptr_-&gt;width_);  depthFramePtr_new-&gt;setId(TS_obs_.second.id_);  depthFramePtr_new-&gt;setTransformation(TS_obs_.second.tr_);  depthFramePtr_ = depthFramePtr_new;DepthFrame 是一个结构体，它代表了一个深度图帧。它包含一个指向 DepthMap 的智能指针，一个 ID，以及一个表示深度图帧在世界坐标系下的变换矩阵。struct DepthFrame{  EIGEN_MAKE_ALIGNED_OPERATOR_NEW  typedef std::shared_ptr&lt;DepthFrame&gt; Ptr;  DepthFrame(size_t row, size_t col)  {    dMap_ = std::make_shared&lt;DepthMap&gt;(row, col);    id_ = 0;    T_world_frame_.setIdentity();  }  void setId(size_t id)  {    id_ = id;  }  void setTransformation(Transformation &amp;T_world_frame)  {    T_world_frame_ = T_world_frame;  }  void clear()  {    dMap_-&gt;reset();    id_ = 0;    T_world_frame_.setIdentity();  }  DepthMap::Ptr dMap_;  size_t id_;  Transformation T_world_frame_;};vEMP 是用来传递匹配结果的，vEMP 的类型是 std::vector&lt;esvo_core::core::EventMatchPair&gt;  std::vector&lt;EventMatchPair&gt; vEMP;// the container that stores the result of BM.EventMatchPair 结构体定义在 esvo_core\\include\\esvo_core\\container\\EventMatchPair.hstruct EventMatchPair{  EIGEN_MAKE_ALIGNED_OPERATOR_NEW  EventMatchPair() {}  // raw event coordinate  Eigen::Vector2d x_left_raw_;  // rectified_event coordinate (left, right)  Eigen::Vector2d x_left_, x_right_;  // timestamp  ros::Time t_;  // pose of virtual view (T_world_virtual)  Transformation trans_;  // inverse depth  double invDepth_;  // match cost  double cost_;  // gradient (left)  double gx_, gy_;  // disparity  double disp_;};这里通过 ebm_, 调用了 EventBM::createMatchProblem 和 EventBM::match_all_HyperThread，这两个函数的功能是进行 Block Match，得到左右目的视差。下面对这两个函数进行解读。  // block matching  tt_mapping.tic();  ebm_.createMatchProblem(&amp;TS_obs_, &amp;st_map_, &amp;vDenoisedEventsPtr_left_);  ebm_.match_all_HyperThread(vEMP);CreatMatchProblem这里定义了一个名为 createMatchProblem的函数，它接受三个参数：StampedTimeSurfaceObs * pStampedTsObs，StampTransformationMap * pSt_map和 std::vector&lt;dvs_msgs::Event *&gt;* pvEventsPtr。在函数内部，它将这些参数存储在类成员变量中，并为每个事件设置了一个视差搜索范围。因此，可以说这个函数创建了一个匹配问题。void esvo_core::core::EventBM::createMatchProblem(  StampedTimeSurfaceObs * pStampedTsObs,  StampTransformationMap * pSt_map,  std::vector&lt;dvs_msgs::Event *&gt;* pvEventsPtr){  pStampedTsObs_ = pStampedTsObs;  pSt_map_ = pSt_map;  size_t numEvents = pvEventsPtr-&gt;size();  vEventsPtr_.clear();  vEventsPtr_.reserve(numEvents);  vEventsPtr_.insert(vEventsPtr_.end(), pvEventsPtr-&gt;begin(), pvEventsPtr-&gt;end());  if(bSmoothTS_)  {    if(pStampedTsObs_)      pStampedTsObs_-&gt;second.GaussianBlurTS(5);  }  vpDisparitySearchBound_.clear();  vpDisparitySearchBound_.reserve(numEvents);  for(size_t i = 0; i &lt; vEventsPtr_.size(); i++)    vpDisparitySearchBound_.push_back(std::make_pair(min_disparity_, max_disparity_));}Block Match (match_all_HyperThread)非线性优化目标函数的使其最小的过程需要初值，ESVO 采用了 ZNCC 块匹配 Block Match 的方法，相较于暴力搜索 Bruteforce Search 更高效。关于 ZNCC 的原理，本博客在之前已经做了详细的说明，你可以点击这里查看：详解零均值归一化：ZNCC在 esvo_core\\src\\core\\EventBM.cpp中：实现了初始化部分求左右目的视差值的代码，函数调用关系如下所示：  全写应为 esvo_core::core::EventBM::match_all_HyperThread，为了排版简写为 EventBM::match_all_HyperThread，后续均如此简写。graph LRsubgraph EventBM  A[match_all_HyperThread]    B[match]   C[match_an_event]   D[epipolarSearching]  E[zncc_cost]  A --&gt; B --&gt; C --&gt; D --&gt; EendMatch an eventmatch_an_event 的核心部分如下：match_an_event 以左目作为 src patch，右目作为 dst patch，沿着极线 epipolar 从右向左，计算两者的 ZNCC cost，找到最小的 cost，即为最佳匹配。在 match_an_event 中 epipolarSearching函数被调用两次，第一次是进行粗略搜索（coarse searching），第二次是进行精细搜索（fine searching）。两者的区别在于搜索的范围和步长不同。在粗略搜索中，搜索的范围是 lowDisparity到 upDisparity，步长为 step_。而在精细搜索中，搜索的范围是从 bestDisp-(step_-1)到 bestDisp+(step_-1)，步长为1。这意味着精细搜索的范围更小，步长更小，因此可以更准确地找到最佳匹配。总的来说，暴力搜索（coarse searching）是为了快速找到可能的匹配，而精细搜索（fine searching）则是为了更准确地找到最佳匹配。bool esvo_core::core::EventBM::match_an_event(  dvs_msgs::Event* pEvent,  std::pair&lt;size_t, size_t&gt;&amp; pDisparityBound,  esvo_core::core::EventMatchPair&amp; emPair){  size_t lowDisparity = pDisparityBound.first;  size_t upDisparity  = pDisparityBound.second;  // rectify and floor the coordinate  Eigen::Vector2d x_rect = camSysPtr_-&gt;cam_left_ptr_-&gt;getRectifiedUndistortedCoordinate(pEvent-&gt;x, pEvent-&gt;y);  // x_rect = [u_rect, v_rect]    // check if the rectified and undistorted coordinates are outside the image plane. (Added by Yi Zhou on 12 Jan 2021)  // x_rect(0) is u_rect, x_rect(1) is v_rect  // ---------&gt; u_rect  // |  // |  // |  // ↓  // v_rect  if(x_rect(0) &lt; 0 || x_rect(0) &gt; camSysPtr_-&gt;cam_left_ptr_-&gt;width_ - 1 ||     x_rect(1) &lt; 0 || x_rect(1) &gt; camSysPtr_-&gt;cam_left_ptr_-&gt;height_ - 1)    return false;  // This is to avoid depth estimation happening in the mask area.  if(camSysPtr_-&gt;cam_left_ptr_-&gt;UndistortRectify_mask_(x_rect(1), x_rect(0)) &lt;= 125)    return false;  // x1 in the left time_surface  Eigen::Vector2i x1(std::floor(x_rect(0)), std::floor(x_rect(1)));  Eigen::Vector2i x1_left_top;  if(!isValidPatch(x1, x1_left_top))    return false;  // extract the template patch in the left time_surface  Eigen::MatrixXd patch_src = pStampedTsObs_-&gt;second.TS_left_.block(    x1_left_top(1), x1_left_top(0), patch_size_Y_, patch_size_X_);  if((patch_src.array() &lt; 1).count() &gt; 0.95 * patch_src.size())  {//    LOG(INFO) &lt;&lt; \"Low info-noise-ratio. @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\";    infoNoiseRatioLowNum_++;    return false;  }  // LOG(INFO) &lt;&lt; \"patch_src is extracted\";  // searching along the epipolar line (heading to the left direction)  double min_cost = ZNCC_MAX_;  Eigen::Vector2i bestMatch;  size_t bestDisp;  Eigen::MatrixXd patch_dst = Eigen::MatrixXd::Zero(patch_size_Y_, patch_size_X_);  // coarse searching  if(!epipolarSearching(min_cost, bestMatch, bestDisp, patch_dst,    lowDisparity, upDisparity, step_,    x1, patch_src, bUpDownConfiguration_))  {//    LOG(INFO) &lt;&lt; \"Coarse searching fails #################################\";    coarseSearchingFailNum_++;    return false;  }  // fine searching  size_t fine_searching_start_pos = bestDisp-(step_-1) &gt;= 0 ? bestDisp-(step_-1) : 0;  if(!epipolarSearching(min_cost, bestMatch, bestDisp, patch_dst,                    fine_searching_start_pos, bestDisp+(step_-1), 1,                    x1, patch_src, bUpDownConfiguration_))  {    // This indicates the local minima is not surrounded by two neighbors with larger cost,    // This case happens when the best match locates over/outside the boundary of the Time Surface.    fineSearchingFailNum_++;//    LOG(INFO) &lt;&lt; \"fine searching fails ...............\";    return false;  }  // transfer best match to emPair  if(min_cost &lt;= ZNCC_Threshold_)  {    emPair.x_left_raw_ = Eigen::Vector2d((double)pEvent-&gt;x, (double)pEvent-&gt;y);    emPair.x_left_ = x_rect;    emPair.x_right_ = Eigen::Vector2d((double)bestMatch(0), (double)bestMatch(1)) ;    emPair.t_ = pEvent-&gt;ts;    double disparity;    if(bUpDownConfiguration_)      disparity = x1(1) - bestMatch(1);    else      disparity = x1(0) - bestMatch(0);    double depth = camSysPtr_-&gt;baseline_ * camSysPtr_-&gt;cam_left_ptr_-&gt;P_(0,0) / disparity;    auto st_map_iter = tools::StampTransformationMap_lower_bound(*pSt_map_, emPair.t_);    if(st_map_iter == pSt_map_-&gt;end())      return false;    emPair.trans_ = st_map_iter-&gt;second;    emPair.invDepth_ = 1.0 / depth; // invDepth_ = 1.0 / depth    emPair.cost_ = min_cost;    emPair.disp_ = disparity;    return true;  }  else  {//    LOG(INFO) &lt;&lt; \"BM fails because: \" &lt;&lt; min_cost &lt;&lt; \" &gt; \" &lt;&lt; ZNCC_Threshold_;    return false;  }}Epipolar Searching// patch_src is left patch, patch_dst is right patchbool esvo_core::core::EventBM::epipolarSearching(  double&amp; min_cost, Eigen::Vector2i&amp; bestMatch, size_t&amp; bestDisp, Eigen::MatrixXd&amp; patch_dst,  size_t searching_start_pos, size_t searching_end_pos, size_t searching_step,  Eigen::Vector2i&amp; x1, Eigen::MatrixXd&amp; patch_src, bool bUpDownConfiguration){  bool bFoundOneMatch = false;  std::map&lt;size_t, double&gt; mDispCost;  // searching along the epipolar line (heading to the left direction)  for(size_t disp = searching_start_pos;disp &lt;= searching_end_pos; disp+=searching_step)  {    Eigen::Vector2i x2;    if(!bUpDownConfiguration)      x2 &lt;&lt; x1(0) - disp, x1(1);  // x2 = [x1(0) - disp, x1(1)]    else      x2 &lt;&lt; x1(0), x1(1) - disp;  // x2 = [x1(0), x1(1) - disp]    Eigen::Vector2i x2_left_top;    if(!isValidPatch(x2, x2_left_top))    {      mDispCost.emplace(disp, ZNCC_MAX_); // ZNCC_MAX_ = 1.0      continue;    }    // extract the template patch in the right time_surface    patch_dst = pStampedTsObs_-&gt;second.TS_right_.block(      x2_left_top(1), x2_left_top(0), patch_size_Y_, patch_size_X_);    double cost = ZNCC_MAX_;    cost = zncc_cost(patch_src, patch_dst, false);    mDispCost.emplace(disp, cost);    if(cost &lt;= min_cost)    {      min_cost = cost;      bestMatch = x2;      bestDisp = disp;    }//    LOG(INFO) &lt;&lt; \"epipolar searching: \" &lt;&lt; disp;  }// 检查在搜索范围内是否存在左右两侧的匹配。// 如果存在，则检查它们的代价是否小于 ZNCC_MAX_，// 如果是，则返回 true，表示找到了至少一个匹配。// 如果不存在，则返回 false，表示没有找到匹配。  if(searching_step &gt; 1)// coarse  {    // mDispCost.end()指向map中不存在的元素    // 确保两端都有匹配    if(mDispCost.find(bestDisp - searching_step) != mDispCost.end() &amp;&amp;       mDispCost.find(bestDisp + searching_step) != mDispCost.end())    {      // 如果两端的匹配成本都小于阈值，则认为找到了一个匹配      if(mDispCost[bestDisp - searching_step] &lt; ZNCC_MAX_ &amp;&amp; mDispCost[bestDisp + searching_step] &lt; ZNCC_MAX_ )        if(min_cost &lt; ZNCC_Threshold_)          bFoundOneMatch = true;//      else//        LOG(INFO) &lt;&lt; \"coarse searching fails: \" &lt;&lt; mDispCost[bestDisp - searching_step] &lt;&lt; \" &lt;-&gt; \"//                  &lt;&lt; mDispCost[bestDisp + searching_step];    }  }  else// fine  {    if(min_cost &lt; ZNCC_Threshold_)      bFoundOneMatch = true;  }  return bFoundOneMatch;}ZNCC costZNCC cost 公式如下：$cost = \\frac{1}{2}(1 - \\frac{\\sum_{i,j}(p_l(i,j)-\\bar{p_l})(p_r(i,j)-\\bar{p_r})}{\\sqrt{\\sum_{i,j}(p_l(i,j)-\\bar{p_l})^2}\\sqrt{\\sum_{i,j}(p_r(i,j)-\\bar{p_r})^2}})$其中:$p_l$和$p_r$分别是左右图像上的图像块$\\bar{p_l}$和$\\bar{p_r}$分别是它们的均值。double esvo_core::core::EventBM::zncc_cost(  Eigen::MatrixXd &amp;patch_left,  Eigen::MatrixXd &amp;patch_right,  bool normalized){  double cost;  // tools::normalizePatch normalizePatch(Eigen::MatrixXd&amp; patch_src,Eigen::MatrixXd&amp; patch_dst)   // patch_src 中的每个元素减去均值，再除以标准差，得到patch_dst。  if(!normalized)  {    Eigen::MatrixXd patch_left_normalized, patch_right_normalized;    tools::normalizePatch(patch_left, patch_left_normalized);    tools::normalizePatch(patch_right, patch_right_normalized);    cost = 0.5 * (1 - (patch_left_normalized.array() * patch_right_normalized.array()).sum() / (patch_left.rows() * patch_left.cols()));  }  else    cost = 0.5 * (1 - (patch_left.array() * patch_right.array()).sum() / (patch_left.rows() * patch_left.cols()));  return cost;}Nonlinear opitmization在 MappingAtTime中，调用了 DepthProblemSolver::solve, 使用非线性最小二乘法优化目标函数 C，得到最优的逆深度 $\\rho^\\star$。  tt_mapping.tic();  // nonlinear opitmization  std::vector&lt;DepthPoint&gt; vdp;  vdp.reserve(vEMP.size());  dpSolver_.solve(&amp;vEMP, &amp;TS_obs_, vdp); // hyper-thread versionDepthPoint 定义在 esvo_core\\include\\esvo_core\\container\\DepthPoint.hSetProblemesvo_core\\src\\core\\DepthProblem.cpp 中的 DepthProblem::operator()定义了待优化的目标函数 C：\\[\\rho^\\star=\\underset{\\rho}{\\operatorname*{\\arg\\min}}C(\\mathbf{x},\\rho,\\mathcal{T}_{\\text{left}} ( \\cdot , t ) , \\mathcal{T}_{\\text{right}} ( \\cdot , t ) , \\mathcal{T}_{t-\\delta t:t})\\]\\[C\\doteq\\sum_{\\mathbf{x}_{1,i}\\in W_1,\\mathbf{x}_{2,i}\\in W_2}r_i^2(\\rho)\\]其中：残差 residual，表征了左右眼的图像块的差异，残差越小，说明两者越相似，即匹配越准确:\\[r_i(\\rho)\\doteq\\mathcal{T}_{\\mathsf{left}}(\\mathbf{x}_{1,i},t)-\\mathcal{T}_{\\mathsf{right}}(\\mathbf{x}_{2,i},t)\\]$x_1$ 与 $x_2$ 的坐标表示如下：\\[\\mathbf{x}_1=\\pi\\big(^{c_t}\\mathbf{T}_{c_{t-\\epsilon}}\\cdot\\pi^{-1}(\\mathbf{x},\\rho_k)\\big)\\]\\[\\mathbf{x}_2=\\pi\\big(^{\\text{right}}\\mathbf{T}_{\\text{left}}\\cdot{}^{c_t}\\mathbf{T}_{c_{t-\\epsilon}}\\cdot\\pi^{-1}(\\mathbf{x},\\rho_k)\\big)\\]逆深度 inverse depth: $\\rho^\\star\\doteq1/Z^\\star $相机轨迹 camera trajectory: $T_t−δt:t$// 将输入向量x作为参数，并计算输出向量fvec// 优化了DepthProblem类中的operator()函数，// 该函数的输入是一个VectorXd类型的向量x，输出是一个VectorXd类型的向量fvec。// 在Levenberg-Marquardt算法中，它将x作为参数输入到operator()函数中，并计算fvec。// 算法的目标是通过调整x的值来最小化fvec的平方和。int DepthProblem::operator()( const Eigen::VectorXd &amp;x, Eigen::VectorXd &amp; fvec ) const{  size_t wx = dpConfigPtr_-&gt;patchSize_X_;  size_t wy = dpConfigPtr_-&gt;patchSize_Y_;  size_t patchSize = wx * wy;  int numValid  = 0;  Eigen::Vector2d x1_s, x2_s;  if(!warping(coordinate_, x(0), vT_left_virtual_[0], x1_s, x2_s))  {    if(strcmp(dpConfigPtr_-&gt;LSnorm_.c_str(), \"l2\") == 0)      for(size_t i = 0; i &lt; patchSize; i++)        fvec[i] = 255;    else if(strcmp(dpConfigPtr_-&gt;LSnorm_.c_str(), \"zncc\") == 0)      for(size_t i = 0; i &lt; patchSize; i++)        fvec[i] = 2 / sqrt(patchSize);    else if(strcmp(dpConfigPtr_-&gt;LSnorm_.c_str(), \"Tdist\") == 0)      for(size_t i = 0; i &lt; patchSize; i++)      {        double residual = 255;        double weight = (dpConfigPtr_-&gt;td_nu_ + 1) / (dpConfigPtr_-&gt;td_nu_ + std::pow(residual / dpConfigPtr_-&gt;td_scale_, 2));        fvec[i] = sqrt(weight) * residual;      }    else      exit(-1);    return numValid;  }void DepthProblem::setProblem(  Eigen::Vector2d &amp; coor,  Eigen::Matrix&lt;double, 4, 4&gt; &amp; T_world_virtual,  StampedTimeSurfaceObs* pStampedTsObs){  coordinate_      = coor;  T_world_virtual_ = T_world_virtual;  pStampedTsObs_ = pStampedTsObs;  vT_left_virtual_.clear();  vT_left_virtual_.reserve(1);  Eigen::Matrix&lt;double,4,4&gt; T_left_world = pStampedTsObs_-&gt;second.tr_.inverse().getTransformationMatrix();  Eigen::Matrix&lt;double,4,4&gt; T_left_virtual = T_left_world * T_world_virtual_;  vT_left_virtual_.push_back(T_left_virtual.block&lt;3,4&gt;(0,0));  resetNumberValues(dpConfigPtr_-&gt;patchSize_X_ * dpConfigPtr_-&gt;patchSize_Y_);}Problem Solvergraph A[DepthProblemSolver::solve]B[DepthProblemSolver::solve_multiple_problems]C[DepthProblemSolver::solve_single_problem_numerical]D[DepthProblem::setProblem]A --&gt; B --&gt; CB --&gt; DSolve Single Problem Numerical一个使用数值优化求解单个深度估计问题的函数。该函数接受一个初始深度估计、一个指向数值微分对象的指针和一个用于存储优化结果的数组。在代码中，创建了一个具有一个元素的 Eigen 向量 “x”，它表示初始深度估计。然后创建了一个 Levenberg-Marquardt 优化器，使用数值微分对象进行初始化，并调用 “lm.minimizeInit” 函数以使用初始深度估计初始化优化器。然后运行优化器，最大迭代次数由 “MAX_ITERATION_” 参数指定。在每次迭代中，调用 “lm.minimizeOneStep” 函数执行一次优化步骤。如果达到最大迭代次数或优化收敛，则终止优化。优化完成后，将深度估计存储在 “result” 数组中。还计算了估计的方差，使用优化的协方差矩阵进行计算，并将结果存储在 “result” 数组中。bool DepthProblemSolver::solve_single_problem_numerical(  double d_init, // 初始深度估计  std::shared_ptr&lt; Eigen::NumericalDiff&lt;DepthProblem&gt; &gt; &amp; dProblemPtr, // 数值微分对象指针  double* result) // 存储优化结果的数组指针{  Eigen::VectorXd x(1); // 创建具有一个元素的 Eigen 向量 x，表示初始深度估计  x &lt;&lt; d_init;  // 创建 Levenberg-Marquardt 优化器 lm，使用数值微分对象进行初始化  // 要优化的目标函数是 DepthProblem 类中的 operator() 函数，  // 它计算了当前深度估计下的重投影误差。  // 这个函数的输入是一个深度值，输出是重投影误差。  // 优化器的目标是最小化这个函数的输出，以得到最优的深度估计。  Eigen::LevenbergMarquardt&lt;Eigen::NumericalDiff&lt;DepthProblem&gt;, double&gt; lm(*(dProblemPtr.get()));  lm.resetParameters();  lm.parameters.ftol = 1e-6; // 设置函数值变化的容忍度  lm.parameters.xtol = 1e-6; // 设置参数变化的容忍度  lm.parameters.maxfev = dpConfigPtr_-&gt;MAX_ITERATION_ * 3; // 设置最大迭代次数  // 使用初始深度估计初始化优化器  if(lm.minimizeInit(x) == Eigen::LevenbergMarquardtSpace::ImproperInputParameters)  {    LOG(ERROR) &lt;&lt; \"ImproperInputParameters for LM (Mapping).\" &lt;&lt; std::endl;    return false;  }  size_t iteration = 0;  int optimizationState = 0;  // 运行优化器，最大迭代次数由 dpConfigPtr_-&gt;MAX_ITERATION_ 参数指定  while(true)  {    Eigen::LevenbergMarquardtSpace::Status status = lm.minimizeOneStep(x); // 执行一次优化步骤    iteration++;    if(iteration &gt;= dpConfigPtr_-&gt;MAX_ITERATION_)      break;    bool terminate = false;    if(status == 2 || status == 3) // 判断优化状态    {      switch (optimizationState)      {        case 0:        {          optimizationState++;          break;        }        case 1:        {          terminate = true;          break;        }      }    }    if(terminate)      break;  }  // 由于 Eigen 中没有设置参数边界的方法，因此在此应用方便的异常值拒绝方法  if(x(0) &lt;= 0.001) // 如果深度估计小于等于 0.001，则返回 false    return false;  // 更新结果数组  result[0] = x(0);  // 计算方差  Eigen::internal::covar(lm.fjac, lm.permutation.indices());  if(dpConfigPtr_-&gt;LSnorm_ == \"l2\") // 如果使用 L2 范数  {    double fnorm = lm.fvec.blueNorm();    double covfac = fnorm * fnorm / (dProblemPtr-&gt;values() - dProblemPtr-&gt;inputs());    Eigen::MatrixXd cov = covfac * lm.fjac.topLeftCorner&lt;1,1&gt;();    result[1] = cov(0,0);  }  if(dpConfigPtr_-&gt;LSnorm_ == \"Tdist\") // 如果使用 T 分布  {    Eigen::MatrixXd invSumJtT = lm.fjac.topLeftCorner&lt;1,1&gt;();    result[1] = std::pow(dpConfigPtr_-&gt;td_stdvar_,2) * invSumJtT(0,0);  }  result[2] = lm.fnorm * lm.fnorm; // 计算残差平方和  return true;}Tracking  TODO: TrackingReference            ZHOU Y, GALLEGO G, SHEN S. “Event-based Stereo Visual Odometry,” IEEE Transactions on Robotics, 37(5): 1433-1450, 2021. DOI:10.1109/TRO.2021.3062252. &#8617;              S.-H. Ieng, J. Carneiro, M. Osswald, and R. Benosman, “Neuromorphic event-based generalized time-based stereovision,” Front. Neurosci., vol. 12, p. 442, 2018. &#8617;              H. Hirschmuller, “Stereo processing by semiglobal matching and mutual information,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 2, pp. 328–341, Feb. 2008. &#8617;      "
  },
  
  {
    "title": "事件相机图像重构：浅谈Time-Surface",
    "url": "/posts/timesurface/",
    "categories": "TecDoc, Paper Reading",
    "tags": "Event-based vision, Time-surface",
    "date": "2023-10-12 23:33:33 +0800",
    





    
    "snippet": "事件相机简介事件相机只输出亮度变化超过一定阈值的像素坐标、时间戳与极性。因为事件流是异步产生的，与传统的固定事件曝光的图像相比富含更时空信息，所以将事件流转换为图像帧的算法至关重要，选对了算法就能发挥事件相机的高时间分辨率优势。事件相机与传统的高速相机相比，输出的事件流中不含冗余的背景图像信息，事件流只输出亮度变化超过阈值的运动物体的信息，这是我们感兴趣的，因此提高了处理速度，降低了数据量，...",
    "content": "事件相机简介事件相机只输出亮度变化超过一定阈值的像素坐标、时间戳与极性。因为事件流是异步产生的，与传统的固定事件曝光的图像相比富含更时空信息，所以将事件流转换为图像帧的算法至关重要，选对了算法就能发挥事件相机的高时间分辨率优势。事件相机与传统的高速相机相比，输出的事件流中不含冗余的背景图像信息，事件流只输出亮度变化超过阈值的运动物体的信息，这是我们感兴趣的，因此提高了处理速度，降低了数据量，避免使用过多的硬件资源，更利于实时计算。事件相机的一般输出形式是事件流，事件流可以表示成如下的形式：$ev_i=[\\mathbf{x_i},t_i,p_i]^T,\\quad i\\in\\mathbb{N},$$ev_i$是第i个事件，其坐标为$\\mathbf{x_i}=[x_i,y_i]^T$时间戳$t_i$，极性$ p_{i}\\in{-1,1}$，-1表示OFF事件，1表示ON事件像素异步生成事件，形成时空点云，代表对象的空间分布和动态行为。Time-Surface[1]Time-Surface 是一种重构事件流的方法，它将事件流转换为图像帧，使得事件流的信息可以用传统的计算机视觉算法处理. Time-Surface 的核心思想是将事件流的时间信息转换为空间信息，将事件流的时间戳$t_i$转换为像素的灰度，这样就可以将事件流转换为图像帧。[1]以 $\\mathbf{x_i}=[x_i,y_i]^T$ 为中心的一个 $2R+1\\times2R+1$ 的窗口:可以表示为: $ T_i(\\mathbf{u},p)=\\max_{j\\leq i}{t_j|\\mathbf{x_j}=(\\mathbf{x_i}+\\mathbf{u}),p_j=p} $其中 $T_i(\\mathbf{u},p)$ 是 time-context，时间上下文$\\mathbf{u}=\\begin{bmatrix}u_x,u_y\\end{bmatrix}^T$ 是窗口中的像素, $u_x\\in{-R,\\ldots,R}$ $u_y\\in{-R,\\ldots,R}$Time-Surface 中的每个像素的灰度值度是时间值的编码：亮像素显示最近的活动，而暗像素接收过去更远的事件（为了清楚起见，图中仅表示与 OFF 事件相对应的时间值）$\\mathcal{S}_i(\\mathbf{u},p)$ 是 $ev_i$ 附近的的 time-surface:$\\mathcal{S}_i(\\mathbf{u},p)=e^{-(t_i-\\mathcal{T}_i(\\mathbf{u},p))/\\tau}$Time-Surface 提供了动态的时空上下文信息,指数衰减系数扩展了时间邻域中的信息三维视角下的 Time-Surface:Time-Surface Prototypes为了构建整个平面而不是窗口的局部 Time-Surface, 这里使用 incremental clustering process (增量聚类)生成 Time-Surface.  [1] LAGORCE X, ORCHARD G, GALLUPPI F, 等. HOTS: A Hierarchy of Event-Based Time-Surfaces for Pattern Recognition[J/OL]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39(7): 1346-1359. DOI:10.1109/TPAMI.2016.2574707.速度不变的 Time-Surface[2]A common representation used in event-based vision is the Surface of Active Events [7], also referred as Time Surface [30]. The Time Surface T at a pixel (x, y) and polarity p is defined as T (x, y, p) ← t, (2) where t is the time of the last event with polarity p occurred at pixel (x, y).Time-Surface 的局部可能有很大的变化。事实上，根据速度、方向和拐角的对比度，Time-Surface 的可能会有很大变化。为了保持分类模型的紧凑和高效，对其输入引入一些归一化非常重要。为了角点检测角点,应当使用相对时间而不是绝对的时间戳.然而在每个事件发生时更新一次局部的 Time-Surface 太过于昂贵,而且存储每个像素的多个时间戳,消耗资源.若周围更高,则把S(x,y,p)周围的都削一圈,把S(x,y,p)赋值为(2r+1)^2(那就是把(2r+1)^2减1?)  [2]MANDERSCHEID J, SIRONI A, BOURDIS N. Speed Invariant Time Surface for Learning to Detect Corner Points With Event-Based Cameras[C/OL]//2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Long Beach, CA, USA: IEEE, 2019: 10237-10246[2023-10-10]. https://ieeexplore.ieee.org/document/8954376/. DOI:10.1109/CVPR.2019.01049."
  },
  
  {
    "title": "Prophesee Mono Calibration",
    "url": "/posts/mono_calibration/",
    "categories": "TecDoc",
    "tags": "Event-based Vision, Calibration",
    "date": "2023-09-19 23:33:33 +0800",
    





    
    "snippet": "Intrinsics Calibration建立相机成像几何模型并矫正透镜畸变。使用基于事件的2D角点检测对相机进行标定。  首先对闪烁棋牌格进行检测：代码路径如下所示：  &lt;install-prefix&gt;/share/metavision/sdk/calibration/apps/metavision_mono_calibration_recording  使用检测数据进行相机...",
    "content": "Intrinsics Calibration建立相机成像几何模型并矫正透镜畸变。使用基于事件的2D角点检测对相机进行标定。  首先对闪烁棋牌格进行检测：代码路径如下所示：  &lt;install-prefix&gt;/share/metavision/sdk/calibration/apps/metavision_mono_calibration_recording  使用检测数据进行相机内参计算：代码路径如下所示：  &lt;install-prefix&gt;/share/metavision/sdk/calibration/apps/metavision_mono_calibration闪烁棋盘格与标定界面代码流水线结构Trail Stage使用 Metavision::TrailFilterAlgorithmT作用是降噪、减少事件数量，Filter 的时间窗口很宽，所以事件要么是极性改变要么是新的事件，而不是同一事件多次输出。Pattern Detector StageMetavision::CalibrationDetectionResult对事件进行关键点检测，同时过滤掉边缘不准确的关键点。Pattern Detector 有两种：  Metavision::BlinkingFrameGeneratorAlgorithm  Metavision::BlinkingDotsGridDetectorAlgorithmLogger Stage收集检测结果，记录为 json 文件。Pattern Frame Generator StageMetavision::CalibrationDetectionFrameGenerator生成包含 pattern detection 的图像帧。Frame Generation StageMetavision::PeriodicFrameGenerationAlgorithm根据事件生成图像帧并发送到下一 stage.Frame Composition StageMetavision::FrameCompositionStage并排显示到目前为止生成的两个帧：由帧生成器阶段生成的事件帧和由模式帧生成器阶段产生的检测帧。Display Stage计算内参metavision_mono_calibration 使用 metavision_mono_calibration_recording 的 2Dpattern detector 生成相机内参。在 non-fronto parallel distorted 输入图像的情况下，准确估计定义校准图案的几何形状的关键点可能会出错。该应用程序允许使用额外的细化步骤，该步骤使用第一次相机标定的数据估计来将输入图像重投影规范的前向平行图像，然后用该图像于精确定位关键点并重新估计相机参数。输出结果对于针孔相机模型而言：  distortion vector: [k1,k2,p1,p2,k3] ,  radial coefficients: k1, k2, k3  tangential coefficients:p1, p2示例:$metavision_mono_calibration -i /tmp/mono_calibrationTool showing how to use Metavision Calibration SDK to calibrate the intrinsic parameters of the camera.Press 'q' or Escape key to leave the program.Starting Calibration...Calibration done.Kept 50/52 views.RMS reprojection error: 0.725819 pixCamera matrix[1711.009729400656, 0, 644.1701299941008; 0, 1711.009729400656, 340.831295617325; 0, 0, 1]Distortion coefficients[-0.08066012661438592, -0.03151075036817833, -0.00027394542171298, 0.001511891330106499, 0]Pattern 1: 0.998101 pixPattern 2: 0.695803 pixPattern 3: 0.634835 pixPattern 51: 0.771156 pixPattern 52: 0.530244 pixIntrinsics, extrinsics and per view RMS reprojection errors have been saved in /tmp/mono_calibration/intrinsics.jsonReferenceIntrinsics Calibration — Metavision SDK Docs 4.3.0 documentation"
  },
  
  {
    "title": "详解零均值归一化：ZNCC",
    "url": "/posts/NCC_ZNCC/",
    "categories": "TecDoc",
    "tags": "Stereo Vision, ZNCC",
    "date": "2023-09-12 23:33:33 +0800",
    





    
    "snippet": "零均值归一化：ZNCC, 是一种进行双目深度估计的算法，主要思想为匹配左右目零均值归一化系数最小的两点。立体匹配与深度计算非基于深度学习的双目匹配算法大致分为四步：  采集图像  极线矫正  特征匹配  深度恢复NCC 与 ZNCC 是衡量特征匹配的算法，一般采用 WTA 的方法得到视差。双目视觉的几何原理并不是本文的重点，挖个坑以后再填标准差、方差、协方差、相关系数标准差标准差描述了变量在...",
    "content": "零均值归一化：ZNCC, 是一种进行双目深度估计的算法，主要思想为匹配左右目零均值归一化系数最小的两点。立体匹配与深度计算非基于深度学习的双目匹配算法大致分为四步：  采集图像  极线矫正  特征匹配  深度恢复NCC 与 ZNCC 是衡量特征匹配的算法，一般采用 WTA 的方法得到视差。双目视觉的几何原理并不是本文的重点，挖个坑以后再填标准差、方差、协方差、相关系数标准差标准差描述了变量在整体变化过程中偏离均值的幅度。\\[\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(X_i-\\overline{X})^{2}}\\]标准差越小，说明数据越集中。所以在协方差中标准差作为分母对协方差进行了归一化。方差标准差的平方，衡量离散程度，计算每一个变量（观察值）与总体均数之间的差异。\\[Var(X)=\\frac{1}{n}\\sum_{i=1}^{n}(X_i-\\overline{X})^{2}\\]协方差协方差反映两个随机变量相关程度，若两变量变化趋势相同，协方差为正值，反之为负。\\[Cov(X,Y)=E[(X-\\overline{X})(Y-\\overline{Y})]=E[XY]-E(X)E(Y)=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\overline{X})(Y_i-\\overline{Y})\\]相关系数相关系数，亦称作皮尔逊相关系数。相关系数是协方差的归一化(normalization)， 消除了两个变量量纲/变化幅度不同的影响，单纯反映两个变量在每单位变化的相似程度。\\[p=\\frac{Cov(X,Y)}{\\sigma_{x}\\sigma_{y}}\\]或\\[r(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)} }\\]其中：  $X$ 是左图；  $Y$ 是右图；  $Cov(X,Y)$ 是左右两图的协方差；  $Var(X)$ 是左图自己的方差；NCC简介归一化相关性（normalization cross-correlation, NCC）使用类似于卷积核的n*n的匹配窗口对左右图像进行处理 ，检测左右匹配窗口的相似度，NCC 为左右图方框对应的 NCC 平均值。双目匹配需要基线矫正，保证两幅图像只是在水平方向发生位移，产生视差。公式\\[NCC(p,d)=\\frac{\\sum_{(x,y)\\in W_p}^{}I_1(x,y)\\cdot I_2(x+d,y) }{\\sqrt{\\sum_{(x,y)\\in W_p}^{}I_1(x,y)^{2} \\cdot \\sum_{(x,y)\\in W_p}^{}I_2(x,y)^{2}} }\\]其中：$NCC \\in [-1,1]$,若$NCC=-1$，表示完全不相关；若 $NCC=1$，表示完全匹配  $I_1$是左图，$I_2$是右图;  $W_p$是以待匹配像素坐标为中心的匹配窗口；  $I_1(x,y)$是左图中待匹配像素值，坐标$(p_x,p_y)$；  $I_2(x+d,y)$是右图对应位置x坐标偏移d像素后的像素值，红框表示；ZNCC简介零均值归一化(ZNCC, Zero-normalized cross-correlation), 相较 NCC 更鲁棒，因为 ZNCC 公式里减去了窗口内的均值，更能抵御光照的变化.  NCC 的公式记成 ZNCC 的公式，导致了很多误解。其实 NCC 的公式应用的特别少，多数使用的是 ZNCC 的公式，只不过有些人将 ZNCC 与 NCC 的名称弄混淆，导致 NCC 这个名词的知名度很高，其实多数使用的公式却是 ZNCC. [3]公式\\[ZNCC(p,d)=\\frac{\\sum_{(x,y)\\in W_p}^{}(I_1(x,y)-\\overline{I_1}(p_x,p_y)\\cdot (I_2(x,y)-\\overline{I_2}(p_x+d,p_y) ) }{\\sqrt{\\sum_{(x,y)\\in W_p}^{}(I_1(x,y)-\\overline{I_1}(p_x,p_y))^{2} \\cdot \\sum_{(x,y)\\in W_p}^{}(I_2(x,y)-\\overline{I_2}(p_x+d,p_y))^{2}} }\\]其中：  $I_1$是左图，$I_2$是右图;  $W_p$是以待匹配像素坐标为中心的匹配窗口；  $I_1(x,y)$是左图中待匹配像素值，坐标$(p_x,p_y)$；  $I_2(x+d,y)$是右图对应位置x坐标偏移d像素后的像素值，红框表示；  $\\overline{I}_1(p_x,p_y)$是左图匹配窗口所有像素均值；  $\\overline{I}_2(p_x+d,p_y)$是右图匹配所有窗口像素均值；NCC与ZNCC的关系与向量理解[4]更优雅的公式NCCNCC与ZNCC的唯一区别是不减去匹配窗口的均值亮度。\\[\\frac{1}{n}\\sum_{x,y}^{}\\frac{1}{\\sigma_f\\sigma_t}f(x,y)t(x,y)\\]ZNCC$t(x,y)$：模板图像 待检测的图像，可以简单理解为右图，检测过程为在右图中寻找与子图像：$f(x,y)$的互相关被称为ZNCC，公式如下所示:\\[\\frac{1}{n}\\sum_{x,y}^{}\\frac{1}{\\sigma_f\\sigma_t}(f(x,y)-\\mu_f)(t(x,y)-\\mu_t)\\]其中：  $n$ is the number of pixels in $t(x,y)$ and $f(x,y)$  $\\sigma_f$ and $\\sigma_t$ are the standard deviations of $f(x,y)$ and $t(x,y)$  $\\mu_f$ and $\\mu_t$ are the average of $f(x,y)$ and $t(x,y)$以向量的角度在泛函分析中，ZNCC可以被看作是两个归一化向量的内积(dot product)That is, if$F(x,y)=f(x,y)-\\mu_f$and$T(x,y)=t(x,y)-\\mu_t$then the ZNCC is the normalized dot product of the two vectors:$\\left \\langle \\frac{F}{\\left | F \\right | },\\frac{T}{\\left | T \\right | } \\right \\rangle$因此，如果$f$和$t$是实矩阵，那么ZNCC相当于$F$与$T$所对应的单位向量夹角的余弦值,两向量之间的交角为0即余弦值为1时即两向量方向相同，反之相反It is also the 2-dimensional version of Pearson product-moment correlation coefficient.where:  $\\left | \\cdot \\right |$  is the  $L^2$norm.  $\\left \\langle \\cdot,\\cdot \\right \\rangle $ is the inner product.  Cauchy-Schwarz implies that the ZNCC has a range of $[-1,1]$.  $L^2$ norm is the Euclidean norm, and is defined as the sum of the squares of the vector elements.  $\\left | F \\right |=\\sqrt{\\sum_{1}^{n}(f(x,y)-\\mu_f)}$  $\\left | F \\right | \\left | T \\right | =n\\sigma_f\\sigma _t$  And cos is:  $\\cos \\theta=\\frac{a \\cdot b}{\\left | a \\right | \\left | b \\right |}$ImplementationC++代码包含ZNCC和SAD，SAD的代码借鉴于 https://zhuanlan.zhihu.com/p/165405979#include\"iostream\"#include\"opencv2/opencv.hpp\"#include\"iomanip\"#include\"cmath\"using namespace std;using namespace cv;class SAD{\tpublic:\tSAD() :winSize(7), DSR(30), threshold(10){}\tSAD(int _winSize, int _DSR, int _threshold) :winSize(_winSize), DSR(_DSR), threshold(_threshold){}\tMat computerSAD(Mat &amp;L, Mat &amp;R); // 计算SAD\tMat computerZNCC(Mat &amp;L, Mat &amp;R); //计算ZNCC\tprivate:\tint winSize; //卷积核的尺寸\tint DSR; //视差搜索范围\tint threshold;}; Mat SAD::computerZNCC(Mat &amp;L, Mat &amp;R){\tint Height = L.rows;\tint Width = L.cols;\tMat Kernel_L(Size(winSize, winSize), CV_8U, Scalar::all(0));\tMat Kernel_R(Size(winSize, winSize), CV_8U, Scalar::all(0));\tMat Disparity(Height, Width, CV_8U, Scalar(0)); //视差图\tfor (int i = 0; i &lt; Width - winSize; i++) {\t\tfor (int j = 0; j &lt; Height - winSize; j++) {\t\t\tMat MM(1, DSR, CV_32F, Scalar(0));\t\t\tif(L.at&lt;uchar&gt;(j,i) &gt; threshold) \t\t\t{\t\t\t\tKernel_L = L(Rect(i, j, winSize, winSize));\t\t\t\tfor (int k = 0; k &lt; DSR; k++) {\t\t\t\t\tint x = i - k;\t\t\t\t\tif (x &lt;= Width - winSize &amp;&amp; x &gt;=0) {\t\t\t\t\t\tKernel_R = R(Rect(x, j, winSize, winSize));\t\t\t\t\t\tScalar mean_L = mean(Kernel_L);\t\t\t\t\t\tScalar mean_R = mean(Kernel_R);\t\t\t\t\t\tint64_t  sum_upper = 0;\t\t\t\t\t\tint64_t  square_sum_L = 0;\t\t\t\t\t\tint64_t  square_sum_R = 0;\t\t\t\t\t\tfor (int a = j; a &lt; j + winSize; a++) {\t\t\t\t\t\t\tfor (int b = i; b &lt; i + winSize; b++) {\t\t\t\t\t\t\t\tsum_upper += (L.at&lt;char&gt;(a, b) - mean_L[0]) * ((R.at&lt;char&gt;(a, b-k) - mean_R[0]));\t\t\t\t\t\t\t\tsquare_sum_L += (L.at&lt;char&gt;(a, b) - mean_L[0]) * ((L.at&lt;char&gt;(a, b) - mean_L[0]));\t\t\t\t\t\t\t\tsquare_sum_R += (R.at&lt;char&gt;(a, b-k) - mean_R[0]) * ((R.at&lt;char&gt;(a, b-k) - mean_R[0]));\t\t\t\t\t\t\t}\t\t\t\t\t\t}\t\t\t\t\t\t// cout &lt;&lt; \"Kernel_L\" &lt;&lt; endl\t\t\t\t\t\t// \t &lt;&lt; Kernel_L &lt;&lt; endl;\t\t\t\t\t\t// cout &lt;&lt; \"Kernel_R:\" &lt;&lt; endl\t\t\t\t\t\t// \t &lt;&lt; Kernel_R &lt;&lt; endl;\t\t\t\t\t\t// cout &lt;&lt; \"mean_L:\" &lt;&lt; mean_L[0] &lt;&lt; endl;\t\t\t\t\t\t// cout &lt;&lt; \"mean_R:\" &lt;&lt; mean_R[0] &lt;&lt; endl;\t\t\t\t\t\t// cout &lt;&lt; \"sum_upper:\" &lt;&lt; sum_upper &lt;&lt; endl;\t\t\t\t\t\t// cout &lt;&lt; \"square_sum_L:\" &lt;&lt; square_sum_L &lt;&lt; endl;\t\t\t\t\t\t// cout &lt;&lt; \"square_sum_R:\" &lt;&lt; square_sum_R &lt;&lt; endl;\t\t\t\t\t\t// cout &lt;&lt; \"sqrt:\" &lt;&lt; (sqrt(square_sum_L) * sqrt(square_sum_R)) &lt;&lt; endl;\t\t\t\t\t\t// cout &lt;&lt; \"MM:\" &lt;&lt; sum_upper / (sqrt(square_sum_L) * sqrt(square_sum_R)) &lt;&lt; endl;\t\t\t\t\t\tif(sqrt(square_sum_L * square_sum_R) != 0) {\t\t\t\t\t\t\tMM.at&lt;float&gt;(k) = sum_upper / (sqrt(square_sum_L) * sqrt(square_sum_R));\t\t\t\t\t\t}\t\t\t\t\t\t// cout&lt;&lt;\"MM full:\"&lt;&lt;MM&lt;&lt;endl;\t\t\t\t\t\t// cout&lt;&lt;MM.at&lt;float&gt;(k)&lt;&lt;endl;\t\t\t\t\t}\t\t\t\t}\t\t\t}\t\t\tPoint maxLoc;\t\t\tminMaxLoc(MM, NULL, NULL, NULL, &amp;maxLoc);\t\t\tint loc = maxLoc.x;\t\t\t// cout&lt;&lt;\"loc:\"&lt;&lt;loc&lt;&lt;endl;\t\t\t// cout&lt;&lt;loc&lt;&lt;endl;\t\t\t// loc = DSR-loc;\t\t\t// Disparity.at&lt;char&gt;(j, i) = loc * 16;\t\t\t// cout &lt;&lt; \"Disparity:\" &lt;&lt; endl&lt;&lt; Disparity &lt;&lt; endl;\t\t\tDisparity.at&lt;char&gt;(j, i) = loc;\t\t}\t\t// double rate = double(i) / (Width);\t\t// cout &lt;&lt; \"已完成\" &lt;&lt; setprecision(2) &lt;&lt; rate * 100 &lt;&lt; \"%\" &lt;&lt; endl; //显示处理进度\t}\t// cout&lt;&lt;\"Disparity:\"&lt;&lt;endl&lt;&lt;Disparity&lt;&lt;endl;\treturn Disparity;} Mat SAD::computerSAD(Mat &amp;L, Mat &amp;R){\tint Height = L.rows;\tint Width = L.cols;\tMat Kernel_L(Size(winSize, winSize), CV_8U, Scalar::all(0));\tMat Kernel_R(Size(winSize, winSize), CV_8U, Scalar::all(0));\tMat Disparity(Height, Width, CV_8U, Scalar(0)); //视差图\tfor (int i = 0; i &lt; Width - winSize; i++) //左图从DSR开始遍历\t{\t\tfor (int j = 0; j&lt;Height - winSize; j++){\t\t\tif(L.at&lt;uchar&gt;(j,i) &gt; threshold)\t\t\t{\t\t\t\tKernel_L = L(Rect(i, j, winSize, winSize));\t\t\t\tMat MM(1, DSR, CV_32F, Scalar(0)); \t\t\t\tfor (int k = 0; k&lt;DSR; k++)\t\t\t\t{\t\t\t\t\tint x = i - k;\t\t\t\t\tif (x &gt;= 0)\t\t\t\t\t{\t\t\t\t\t\tKernel_R = R(Rect(x, j, winSize, winSize));\t\t\t\t\t\tMat Dif;\t\t\t\t\t\tabsdiff(Kernel_L, Kernel_R, Dif); // 求差的绝对值之和\t\t\t\t\t\tScalar ADD = sum(Dif);\t\t\t\t\t\tfloat a = ADD[0];\t\t\t\t\t\tMM.at&lt;float&gt;(k) = a;\t\t\t\t\t}\t\t\t\t}\t\t\t\tPoint minLoc;\t\t\t\tminMaxLoc(MM, NULL, NULL, &amp;minLoc, NULL);\t\t\t\tint loc = minLoc.x;\t\t\t\t// int loc=DSR-loc;\t\t\t\t// Disparity.at&lt;char&gt;(j, i) = loc * 16;\t\t\t\tDisparity.at&lt;char&gt;(j, i) = loc;\t\t\t}\t\t}\t\t// double rate = double(i) / (Width);\t\t// cout &lt;&lt; \"已完成\" &lt;&lt; setprecision(2) &lt;&lt; rate * 100 &lt;&lt; \"%\" &lt;&lt; endl; // 显示处理进度\t}\treturn Disparity;}int main(int argc, char* argv[]){\tMat Img_L = imread(\"im2.png\", 0); \tMat Img_R = imread(\"im6.png\", 0);\t// Mat Img_L = imread(\"m_denoise_l.png\", 0); \t// Mat Img_R = imread(\"m_denoise_r.png\", 0);\tMat Disparity; //创建视差图\tSAD myZNCC(7, 40, 1); //参数 winSize DSR threshold\tSAD mySAD(7, 30, 10);\tDisparity = myZNCC.computerZNCC(Img_L, Img_R);\t// Disparity = mySAD.computerSAD(Img_L, Img_R);\t// imshow(\"Teddy_L\", Img_L);\t// imshow(\"Teddy_R\", Img_R);\timshow(\"Disparity\", Disparity); //显示视差图\twaitKey();\timwrite(\"Disparity.png\", Disparity);\t// system(\"pause\");  //按任意键退出\treturn 0;}Python边缘像素如何处理?为了简化计算，这里并没有对图像进行裁切边缘的操作。 scipy.ndimage.uniform_filter 函数是一个多维均匀滤波器，滤波器窗口大小为 size 参数指定，默认为3.使用改函数进行匹配窗口的平均值计算，该函数的 mode 参数可以设置边缘情况的处理方式。scipy.ndimage.uniform_filter(input, size=3, output=None, mode='reflect', cval=0.0, origin=0, *, axes=None)  The mode parameter determines how the input array is extended when the filter overlaps a border. By passing a sequence of modes with length equal to the number of dimensions of the input array, different modes can be specified along each axis. Default value is ‘reflect’. The valid values and their behavior is as follows:      ‘reflect’ (d c b a |a b c d | d c b a)The input is extended by reflecting about the edge of the last pixel. This mode is also sometimes referred to as half-sample symmetric.    ‘constant’ (k k k k |a b c d| k k k k)The input is extended by filling all values beyond the edge with the same constant value, defined by the cval parameter.    ‘nearest’ (a a a a | a b c d| d d d d)The input is extended by replicating the last pixel.  NCCfrom PIL import Imagefrom pylab import *import cv2from numpy import *from numpy.ma import arrayfrom scipy.ndimage import filtersimport scipy.miscimport imageio  def plane_sweep_ncc(im_l, im_r, start, steps, wid):    \"\"\" 使用归一化的互相关计算视差图像 \"\"\"    m, n = im_l.shape    # 创建保存不同求和值的数组    mean_l = zeros((m, n))    mean_r = zeros((m, n))    s = zeros((m, n))    s_l = zeros((m, n))    s_r = zeros((m, n))    # 创建保存深度平面的数组    dmaps = zeros((m, n, steps))    # # only for ZNCC    # # 计算图像块的平均值    # filters.uniform_filter(im_l, wid, mean_l)    # filters.uniform_filter(im_r, wid, mean_r)    # # 归一化图像    # norm_l = im_l - mean_l    # norm_r = im_r - mean_r    # 尝试不同的视差    #  steps 是视差的最大可能值    for displ in range(steps):        # 将左边图像移动到右边，计算加和        # np.roll() 函数将数组元素向后移动 -displ - start 个单位        # 和归一化, s 为 ncc 的分子        filters.uniform_filter(np.roll(norm_l, -displ - start) * norm_r, wid, s)            filters.uniform_filter(np.roll(norm_l, -displ - start) * np.roll(norm_l, -displ - start), wid, s_l)        filters.uniform_filter(norm_r * norm_r, wid, s_r)  # 和反归一化        # 保存 ncc 的值在 dmaps 中        dmaps[:, :, displ] = s / sqrt(s_l * s_r)        # 为每个像素选取最佳深度        # 选取最大的的 dmaps ，即视差，WTA    return np.argmax(dmaps, axis=2)# img is size: 1800 x 1500im_l = array(Image.open(r'D:/Dataset/teddyF/im2.ppm').convert('L'), 'f')im_r = array(Image.open(r'D:/Dataset/teddyF/im6.ppm').convert('L'), 'f')# 参数：与图像尺寸有关，需要根据图像尺寸调整# 视差的最大值steps = 180# 匹配窗口起始偏置，一般为0，因为视差不可能为负start = 0# 匹配窗口的宽度wid = 20 res = plane_sweep_ncc(im_l, im_r, start, steps, wid)imshow(res)show()ZNCCPixelwise ZNCC, 适用于处理事件帧。import cv2import numpy as npclass SAD:\tdef __init__(self, winSize=7, DSR=30, threshold=10):\t\tself.winSize = winSize\t\tself.DSR = DSR\t\tself.threshold = threshold\tdef computerZNCC(self, L, R):\t\tHeight, Width = L.shape\t\tKernel_L = np.zeros((self.winSize, self.winSize), dtype=np.uint8)\t\tKernel_R = np.zeros((self.winSize, self.winSize), dtype=np.uint8)\t\tDisparity = np.zeros((Height, Width), dtype=np.uint8)\t\tfor i in range(Width - self.winSize):\t\t\tfor j in range(Height - self.winSize):\t\t\t\tMM = np.zeros((1, self.DSR), dtype=np.float32)\t\t\t\tif L[j, i] &gt; self.threshold:\t\t\t\t\tKernel_L = L[j:j+self.winSize, i:i+self.winSize]\t\t\t\t\tfor k in range(self.DSR):\t\t\t\t\t\tx = i - k\t\t\t\t\t\tif x &lt;= Width - self.winSize and x &gt;= 0:\t\t\t\t\t\t\tKernel_R = R[j:j+self.winSize, x:x+self.winSize]\t\t\t\t\t\t\tmean_L = np.mean(Kernel_L)\t\t\t\t\t\t\tmean_R = np.mean(Kernel_R)\t\t\t\t\t\t\tsum_upper = 0\t\t\t\t\t\t\tsquare_sum_L = 0\t\t\t\t\t\t\tsquare_sum_R = 0\t\t\t\t\t\t\tfor a in range(j, j+self.winSize):\t\t\t\t\t\t\t\tfor b in range(i, i+self.winSize):\t\t\t\t\t\t\t\t\tsum_upper += (L[a, b] - mean_L) * (R[a, b-k] - mean_R)\t\t\t\t\t\t\t\t\tsquare_sum_L += (L[a, b] - mean_L) ** 2\t\t\t\t\t\t\t\t\tsquare_sum_R += (R[a, b-k] - mean_R) ** 2\t\t\t\t\t\t\tif np.sqrt(square_sum_L * square_sum_R) != 0:\t\t\t\t\t\t\t\tMM[0, k] = sum_upper / np.sqrt(square_sum_L * square_sum_R)\t\t\t\t\t\t\tmin_val,max_val,min_indx,max_indx = cv2.minMaxLoc(MM)\t\t\t\t\t\t\tloc = np.where(MM == max_val)\t\t\t\t\t\t\tDisparity[j, i] = loc[1][0]\t\treturn Disparityif __name__ == '__main__':\tImg_L = cv2.imread(\"m_denoise_l.png\", 0)\tImg_R = cv2.imread(\"m_denoise_r.png\", 0)\tmyZNCC = SAD(7, 40, 130)\tDisparity = myZNCC.computerZNCC(Img_L, Img_R)\t# cv2.imshow(\"Disparity\", Disparity)\t# cv2.waitKey()\tcv2.imwrite(\"Disparity.png\", Disparity)from PIL import Imagefrom pylab import *import cv2from numpy import *from numpy.ma import arrayfrom scipy.ndimage import filtersimport scipy.miscimport imageio  def plane_sweep_ncc(im_l, im_r, start, steps, wid):    \"\"\" 使用归一化的互相关计算视差图像 \"\"\"    m, n = im_l.shape    # 创建保存不同求和值的数组    mean_l = zeros((m, n))    mean_r = zeros((m, n))    s = zeros((m, n))    s_l = zeros((m, n))    s_r = zeros((m, n))    # 创建保存深度平面的数组    dmaps = zeros((m, n, steps))    # 计算图像块的平均值    filters.uniform_filter(im_l, wid, mean_l)    filters.uniform_filter(im_r, wid, mean_r)    # 归一化图像    norm_l = im_l - mean_l    norm_r = im_r - mean_r    # 尝试不同的视差    #  steps 是视差的最大可能值    for displ in range(steps):        # 将左边图像移动到右边，计算加和        # np.roll() 函数将数组元素向后移动 -displ - start 个单位        # 和归一化, s 为 ncc 的分子        filters.uniform_filter(np.roll(norm_l, -displ - start) * norm_r, wid, s)            filters.uniform_filter(np.roll(norm_l, -displ - start) * np.roll(norm_l, -displ - start), wid, s_l)        filters.uniform_filter(norm_r * norm_r, wid, s_r)  # 和反归一化        # 保存 ncc 的值在 dmaps 中        dmaps[:, :, displ] = s / sqrt(s_l * s_r)        # 为每个像素选取最佳深度        # 选取最大的的 dmaps ，即视差，WTA    return np.argmax(dmaps, axis=2)# img is size: 1800 x 1500im_l = array(Image.open(r'D:/Dataset/teddyF/im2.ppm').convert('L'), 'f')im_r = array(Image.open(r'D:/Dataset/teddyF/im6.ppm').convert('L'), 'f')# ZNCC参数：与图像尺寸有关，需要根据图像尺寸调整# 视差的最大值steps = 180# 匹配窗口起始偏置，一般为0，因为视差不可能为负start = 0# 匹配窗口的宽度wid = 20 res = plane_sweep_ncc(im_l, im_r, start, steps, wid)imshow(res)show()Result and groundtruthDatasetsMiddleburry 2003 Stereo datasets with ground truth使用teddyF/im2.ppm和teddyF/im6.ppm作为测试图像。Output resultPython匹配窗口宽度25, 视差图中存在缺陷，需要后续处理，初步验证了NCC和ZNCC算法的正确性。右边缘由于没有进行裁切，所以边缘的视差值不准确Ground truthReference[1]标准差、方差、协方差、相关系数[2]协方差和相关性的意义[3]NCC与ZNCC[4]Cross-correlation_Wikipedia[5]scipy.ndimage.uniform_filter[6]关于双目立体视觉的三大基本算法及发展现状的总结"
  },
  
  {
    "title": "RPG_EMVS",
    "url": "/posts/rpg_emvs/",
    "categories": "TecDoc",
    "tags": "Event-based Vision, EMVS",
    "date": "2023-09-11 23:33:33 +0800",
    





    
    "snippet": "IntroductionRPG_EMVS is the implementation of EMVS: Event-based Multi-View Stereo.You can find it here.InstallationClone the responsitycd home\\yousa\\rosCatkinBuild\\srcgit clone git@github.com:uzh-r...",
    "content": "IntroductionRPG_EMVS is the implementation of EMVS: Event-based Multi-View Stereo.You can find it here.InstallationClone the responsitycd home\\yousa\\rosCatkinBuild\\srcgit clone git@github.com:uzh-rpg/rpg_emvs.gitRun samplecd home\\yousa\\rosCatkinBuild\\src\\ros_emvcmkdir sample &amp;&amp; cd samplesource /home/yousa/rosCatkinBuild/devel/setup.zshrosrun mapper_emvs run_emvs --bag_filename=/home/yousa/dataset/shapes_6dof.bag --flagfile=/home/yousa/rosCatkinBuild/src/rpg_emvs/mapper_emvs/cfg/slider_depth.confOutput  in terminal:~/rosCatkinBuild/src/rpg_emvs/sample master ?1 ❯ rosrun mapper_emvs run_emvs --bag_filename=/home/yousa/dataset/shapes_6dof.bag --flagfile=/home/yousa/rosCatkinBuild/src/rpg_emvs/mapper_emvs/cfg/slider_depth.confI0911 14:59:14.897390  3300 data_loading.cpp:62] initial stamp: 1468939993.067416019I0911 14:59:14.956785  3300 depth_vector.hpp:133] Using linear spacing in inverse depthI0911 14:59:14.956879  3300 mapper_emvs.cpp:183] Specified DSI FoV &lt; 10 deg. Will use camera FoV instead.I0911 14:59:14.956899  3300 mapper_emvs.cpp:191] Focal length of virtual camera: 168.629 pixelsI0911 14:59:15.124897  3300 main.cpp:91] Time to evaluate DSI: 112 millisecondsI0911 14:59:15.124984  3300 main.cpp:92] Number of events processed: 478860 eventsI0911 14:59:15.124994  3300 main.cpp:93] Number of events processed per second: 4.27554 Mev/sI0911 14:59:15.125013  3300 main.cpp:95] Mean square = 634.101I0911 14:59:15.157297  3300 main.cpp:142] Saved 1062 data points to pointcloud.pcdIn sample:└── sample    ├── confidence_map.png    ├── depth_colored.png    ├── depth_map.png    ├── dsi.npy    ├── pointcloud.pcd    └── semidense_mask.pngVisualizationPoint cloudOriginal python script has bug in python3.X, so we use pcl-tools or open3D to visualize point cloud.Pcl-toolssudo apt-get install pcl-toolspcl_viewer /home/yousa/rosCatkinBuild/src/rpg_emvs/sample/pointcloud.pcdNotice it is upside down.Open3Dimport open3d as o3dimport numpy as np ply = o3d.io.read_point_cloud('/home/yousa/rosCatkinBuild/src/rpg_emvs/sample/pointcloud.pcd')  o3d.visualization.draw_geometries([ply])Disparity Space Image(DSI)Install visvispip install visvisVisualize DSIIn visualize_dsi_volume.pyin line 22, change t = vv.volshow(vol, renderStyle = 'mip') to t = vv.volshow3(vol, renderStyle = 'mip')import numpy as npimport argparseimport visvis as vvapp = vv.use()if __name__ == \"__main__\":    parser = argparse.ArgumentParser(description='Plot the disparity space image (DSI) using 3D slices')    parser.add_argument('-i', '--input', default='dsi.npy', type=str,                        help='path to the NPY file containing the DSI (default: dsi.npy)')    args = parser.parse_args()    a = vv.gca()    a.daspect = 1, -1, 1    a.daspectAuto = True    vol = np.load(args.input)      # Reorder axis so that the Z axis points forward instead of up    vol = np.swapaxes(vol, 0, 1)    vol = np.flip(vol, axis=0)      # t = vv.volshow(vol, renderStyle = 'mip')    t = vv.volshow3(vol, renderStyle = 'mip')    t.colormap = vv.CM_HOT      app.Run()python3 /home/yousa/rosCatkinBuild/src/rpg_emvs/mapper_emvs/scripts/visualize_dsi_volume.py -i /home/yousa/rosCatkinBuild/src/rpg_emvs/sample/dsi.npypython3 /home/yousa/rosCatkinBuild/src/rpg_emvs/mapper_emvs/scripts/visualize_dsi_slices.py -i /home/yousa/rosCatkinBuild/src/rpg_emvs/sample/dsi.npy"
  },
  
  {
    "title": "This is my fisrt blog",
    "url": "/posts/first_blog/",
    "categories": "Blogging",
    "tags": "HelloWorld",
    "date": "2023-09-08 02:33:33 +0800",
    





    
    "snippet": "HelloWorld!",
    "content": "HelloWorld!"
  }
  
]

