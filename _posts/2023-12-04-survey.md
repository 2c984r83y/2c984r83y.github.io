---
title: 文献调研
author: 2c984r83y
date: 2023-12-04 21:30:00 +0800
categories: [TecDoc, Survey]
tags: [Survey]
pin: false
math: true
mermaid: true
---
> Survey about stereo matching
> {: .prompt-info }

## Normal Camera

### BGNet:Bilateral Grid Learning for Stereo Matching Networks

#### Date

2021/06

#### Conference/Publication

2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)

#### Author

Bin Xu1, Yuhua Xu1,2,∗, Xiaoli Yang1, Wei Jia2, Yulan Guo3
1Orbbec, 2Hefei University of Technology, 3Sun Yat-sen University

#### 解决的问题/创新点

![20231204220145](https://raw.githubusercontent.com/2c984r83y/2c984r83y.github.io/master/images/20231204220145.png)
解决了实时处理速度与精度的矛盾，构造低分辨率4D cost volume, 设计基于双边网格的无参数切片层，从低分辨率 cost volume 中获得边缘保持的高分辨率 cost volume，加速计算。

#### Result

精度：在 KITTI2015 上 2-noc 达到1.81
速度：耗时32.3ms, 31FPS
![20231204215203](https://raw.githubusercontent.com/2c984r83y/2c984r83y.github.io/master/images/20231204215203.png)

#### Comments

bilateral grid 实现了较快的速度，使用卷积构建 cost volume 还是消耗了较多时间(12.2ms in 32.3ms)

---


### ACVNet:Attention Concatenation Volume for Accurate and Efficient Stereo Matching

#### Date

2022/06

#### Conference/Publication

2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)

#### Author

Gangwei Xu1, Junda Cheng1, Peng Guo1 , Xin Yang1,2
1School of EIC, Huazhong University of Science & Technology
2Wuhan National Laboratory for Optoelectronics

#### 解决的问题/创新点

![20231204224245](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204224245.png)
过去的工作构建 cost volume 消耗大量计算资源与时间，文章旨在探索一种更高效、更有效的cost volume形式，既能显著减轻构建 cost aggregation 的负担，又能达到最先进的精度。
提出了一种基于关联线索生成注意理权值(generates attention weights from correlation clues)的 cost volume 构建方法，以抑制冗余信息，增强 concatenation volume 中的匹配相关信息。
为了产生可靠的注意力权重，文章提出了多级自适应补丁匹配(MAPM, multi-level adaptive patch matching)，以对稀疏和无纹理区域的检测。所提出的代价体积被称为注意力拼接体积(attention concatation volume, ACV)， ACV可以无缝嵌入到大多数立体匹配网络中，所得到的网络可以使用更轻量的聚合网络，同时获得更高的精度，例如仅使用聚合网络的1/25参数就可以获得更高的精度。

#### Result

精度：ACV 将 PSMNet 和 GwcNet 的准确率分别提高了 42% 和 39%。ACVNet 在 KITTI 2012 和 KITTI 2015 上排名第2，在 Scene Flow 上排名第2，在ETH3D上排名第3.
![20231204230205](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204230205.png)
速度：0.2s
![20231204230345](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204230345.png)

#### Comments

引入了注意力机制 attention weights from correlation clues

### Fast-ACVNet:Accurate and Efficient Stereo Matching via Attention Concatenation Volume

#### Date

2023/11 Preprint

#### Conference/Publication

IEEE Transactions on Pattern Analysis and Machine Intelligence

#### Author

Gangwei Xu, Yun Wang, Junda Cheng, Jinhui Tang, Xin Yang
School of EIC, Huazhong University of Science & Technology

#### 解决的问题/创新点

![20231204231601](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204231601.png)
文章进一步设计了一个快速版本的ACV，以实现实时性能，命名为Fast -ACVNet，它从低分辨率的相关线索中产生高似然差异假设和相应的注意权重，从而显着降低计算和内存成本，同时保持准确。
Fast-ACV的核心思想包括批量注意传播(VAP)和精细到重要(F2I)策略。
VAP可以自动从插值的相关体积中选择准确的相关值，并将这些准确的相关值传播到具有模糊相关线索的周围像素，F2I可以生成一组具有高似然的差异假设和相应的注意权值，从而显著抑制拼接体积中不可能存在的差异，从而减少时间和内存成本。

#### Result

精度：SOTA on KITTI
速度：由ACVNet的0.2s提升到45ms
![20231204231715](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204231715.png)

#### Comments

提出VAP和F2I策略，显著降低计算和内存成本，同时保持准确。

---

### HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching

#### Date

2021/06

#### Conference/Publication

2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)

#### Author

Vladimir Tankovich Christian H ̈ ane Yinda Zhang Adarsh Kowdle Sean Fanello Sofien Bouaziz
Google

#### 解决的问题/创新点

解决了构建cost volume 的计算和内存成本高的问题。
与最近许多在全成本体积上运行并依赖3D卷积的神经网络方法相反，我们的方法不明确地构建体积，而是依赖于快速的多分辨率初始化步骤、可微分的2D几何传播和扭曲机制来推断视差假设。为了达到较高的精度，网络不仅从几何上推断差异，而且还推断斜面假设，从而更准确地执行几何翘曲和上采样操作。其计算量仅为最先进方法的一小部分。

#### Result

精度：ETH3D排名第3，在Middlebury-v3上所有端到端学习排名第1，在流行的KITTI 2012和2015基准中排名第1
速度：0.02s, 50FPS
![20231204233437](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204233437.png)

#### Comments

基于传统算法的改进，跳过了构建 cost volume 的步骤，显著降低计算和内存成本，提高速度。
但是官方目前开源的 tensorflow model 运行速度是论文中的三倍。
[https://github.com/google-research/google-research/tree/master/hitnet](https://github.com/google-research/google-research/tree/master/hitnet)

> The released models are using default tensoflow ops which causes them to be 3X slower than the same models that use custom CUDA ops. The custom CUDA ops and a version of the models that use them may be released later.

---

### AANet: Adaptive Aggregation Network for Efficient Stereo Matching

#### Date

2020/06

#### Conference/Publication

2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)

#### Author

Haofei Xu Juyong Zhang
University of Science and Technology of China

#### 解决的问题/创新点

![20231205210814](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231205210814.png)
尽管基于学习的立体匹配算法取得了显著进展，但仍有一个关键挑战尚未解决。目前最先进的立体模型大多基于昂贵的3D卷积，立方计算的复杂性和高内存消耗使其在实际应用中部署相当昂贵。
文章创新点是完全取代常用的3D卷积来实现快速的推理速度，同时主要保持相当的精度。为此，文章首先提出了一种基于稀疏点的尺度内成本聚合方法，以缓解视差不连续处众所周知的 edge-fattening issue。进一步，文章用神经网络层近似传统的跨尺度代价聚合算法来处理大型无纹理区域。这两个模块都是简单、轻量级和互补的，可为成本聚合提供一个有效和高效的体系结构。

#### Result

精度：在运行62ms时，我们还在场景流和KITTI数据集上取得了具有竞争力的结果

速度：62ms，比GC-Net快41倍，比PSMNet快4倍，比GANet快38倍
![20231205212209](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231205212209.png)

#### Comments

使用了稀疏卷积+跨尺度融合
`use 4 NVIDIA V100 GPUs (32G) with batch size 64 for training`

---
## Feature match

### SuperGlue: Learning Feature Matching With Graph Neural Networks

#### Date

2020/06

#### Conference/Publication

2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)

#### Author

Paul-Edouard Sarlin1, Daniel DeTone2, Tomasz Malisiewicz2, Andrew Rabinovich2 
1 ETH Zurich 2 Magic Leap, Inc.

#### 解决的问题/创新点
![20231205222054](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231205222054.png)
与传统的手工设计的启发式方法相比，SuperGlue 通过图像对的端到端训练来学习 3D 世界的几何变换和规律性的先验知识。
SuperGlue 通过共同寻找对应点并拒绝不可匹配的点来匹配两组局部特征。通过解决可微的最优传输问题来估计分配，其成本由图神经网络预测。我引入了一种基于注意力的灵活上下文聚合机制，使 SuperGlue 能够联合推理底层 3D 场景和特征分配。 
#### Result
SuperGlue 的性能优于其他学习方法，并在具有挑战性的现实室内和室外环境中的姿态估计任务中取得了最先进的结果。

精度：
![20231205222336](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231205222336.png)
速度：  69 ms/15 FPS (1080 GPU) 

#### Comments
<https://blog.csdn.net/weixin_42730997/article/details/108562592>
SuperGlue对于匹配问题的新看法，是用一个经典的数学问题来定义，即最优运输问题（Optimal Transport）。

### LightGlue: Local Feature Matching at Light Speed

#### Date

2023

#### Conference/Publication

2023 IEEE/CVF International Conference on Computer Vision (ICCV)

#### Author

Philipp Lindenberger1, Paul-Edouard Sarlin1 Marc Pollefeys1,2
1 ETH Zurich 2 Microsoft Mixed Reality & AI Lab

#### 解决的问题/创新点
![20231205223819](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231205223819.png)
论文重新审视了SuperGlue的多个设计决策，这是稀疏匹配领域的最新技术，并得出了简单但有效的改进方法。这些改进使得LightGlue在内存和计算方面更加高效，更准确，并且更容易训练。其中一个关键特性是LightGlue对问题的难度是自适应的：对于直观上易于匹配的图像对，例如具有更大的视觉重叠或有限的外观变化，推理速度更快。
在准确性、效率和训练易用性方面优于现有的SuperGlue。通过对架构进行简单而有效的修改，提出了训练高性能深度特征匹配器的方法。
LightGlue具有自适应的特性，可以根据图像对的难度进行灵活调整。通过预测对应关系并允许模型自省，可以在易于匹配的图像对上实现更快的推理速度，而在具有挑战性的图像对上仍然保持准确性。
LightGlue的应用前景广阔，特别适用于对延迟敏感的应用，如SLAM和基于众包数据的更大场景重建。

#### Result
![20231205223423](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231205223423.png)
![20231205223851](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231205223851.png)

#### Comments

<https://zhuanlan.zhihu.com/p/642459384>  
匹配稀疏的局部特征。借鉴了SuperGlue的成功，论文将注意力机制的能力与匹配问题的见解以及Transformer的最新创新相结合。论文赋予这个模型自省其预测的置信度的能力。这产生了一种优雅的方案，根据每对图像的难度自适应计算量。模型的深度和宽度都是自适应的：1）如果所有预测都已准备好，推理可以在较早的层停止；2）被认为不可匹配的点从进一步的步骤中提前丢弃。最终得到的模型LightGlue比传统的SuperGlue更快、更准确，并且更容易训练。

### LoFTR: Detector-Free Local Feature Matching with Transformers

#### Date
2021/06
#### Conference/Publication
2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
#### Author
Jiaming Sun1,2 Zehong Shen1, Yuang Wang1, Hujun Bao1 Xiaowei Zhou1† 
1Zhejiang University, 2SenseTime Research
#### 解决的问题/创新点
![20231205232835](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231205232835.png)
文章提出了一种新颖的用于局部图像特征匹配的方法。代替了传统的顺序执行图像特征检测，描述和匹配的步骤，本文提出首先在粗粒度上建立逐像素的密集匹配，然后在精粒度上完善精修匹配的算法。与使用cost volume搜索对应关系的稠密匹配方法相比，本文使用了Transformers中的使用自我和交叉注意力层(self and cross attention layers)来获取两个图像的特征描述符。Transformers提供的全局感受野使图像能够在弱纹理区域产生密集匹配（通常情况下在低纹理区域，特征检测器通常难以产生可重复的特征点）。在室内和室外数据集上进行的实验表明，LoFTR在很大程度上优于现有技术
#### Result
116 ms for a 640×480 image pair on an RTX 2080Ti.
![20231205232937](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231205232937.png)
#### Comments


## Event-based
