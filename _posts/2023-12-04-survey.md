---
title: 文献调研
author: 2c984r83y
date: 2023-12-04 21:30:00 +0800
categories: [TecDoc, Survey]
tags: [Survey]
pin: false
math: true
mermaid: true
---
> Survey about stereo matching
{: .prompt-info }

### 1. BGNet:Bilateral Grid Learning for Stereo Matching Networks

#### Date

2021/06

#### Conference/Publication

2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)

#### Author

Bin Xu1, Yuhua Xu1,2,∗, Xiaoli Yang1, Wei Jia2, Yulan Guo3
1Orbbec, 2Hefei University of Technology, 3Sun Yat-sen University

#### 解决的问题/创新点

![20231204220145](https://raw.githubusercontent.com/2c984r83y/2c984r83y.github.io/master/images/20231204220145.png)
解决了实时处理速度与精度的矛盾，构造低分辨率4D cost volume, 设计基于双边网格的无参数切片层，从低分辨率 cost volume 中获得边缘保持的高分辨率 cost volume，加速计算。

#### 性能

精度：在 KITTI2015 上 2-noc 达到1.81
速度：耗时32.3ms, 31FPS
![20231204215203](https://raw.githubusercontent.com/2c984r83y/2c984r83y.github.io/master/images/20231204215203.png)

#### 结论

实现了较快的速度，使用卷积构建 cost volume 还是消耗了较多时间(12.2ms in 32.3ms)

### 2. ACVNet:Attention Concatenation Volume for Accurate and Efficient Stereo Matching

#### Date
2022/06

#### Conference/Publication
2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)

#### Author
Gangwei Xu1, Junda Cheng1, Peng Guo1 , Xin Yang1,2
1School of EIC, Huazhong University of Science & Technology
2Wuhan National Laboratory for Optoelectronics

#### 解决的问题/创新点

![20231204224245](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204224245.png)
过去的工作构建 cost volume 消耗大量计算资源与时间，文章旨在探索一种更高效、更有效的cost volume形式，既能显著减轻构建 cost aggregation 的负担，又能达到最先进的精度。
提出了一种基于关联线索生成注意理权值(generates attention weights from correlation clues)的 cost volume 构建方法，以抑制冗余信息，增强 concatenation volume 中的匹配相关信息。
为了产生可靠的注意力权重，文章提出了多级自适应补丁匹配(MAPM, multi-level adaptive patch matching)，以对稀疏和无纹理区域的检测。所提出的代价体积被称为注意力拼接体积(attention concatation volume, ACV)， ACV可以无缝嵌入到大多数立体匹配网络中，所得到的网络可以使用更轻量的聚合网络，同时获得更高的精度，例如仅使用聚合网络的1/25参数就可以获得更高的精度。
#### 性能
精度：ACV 将 PSMNet 和 GwcNet 的准确率分别提高了 42% 和 39%。ACVNet 在 KITTI 2012 和 KITTI 2015 上排名第2，在 Scene Flow 上排名第2，在ETH3D上排名第3.
![20231204230205](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204230205.png)
速度：0.2s
![20231204230345](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204230345.png)
#### 结论
引入了注意力机制 attention weights from correlation clues

### 3.Fast-ACVNet:Accurate and Efficient Stereo Matching via Attention Concatenation Volume

#### Date
2023/11 Preprint
#### Conference/Publication

IEEE Transactions on Pattern Analysis and Machine Intelligence
#### Author
Gangwei Xu, Yun Wang, Junda Cheng, Jinhui Tang, Xin Yang
School of EIC, Huazhong University of Science & Technology
#### 解决的问题/创新点
![20231204231601](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204231601.png)
文章进一步设计了一个快速版本的ACV，以实现实时性能，命名为Fast -ACVNet，它从低分辨率的相关线索中产生高似然差异假设和相应的注意权重，从而显着降低计算和内存成本，同时保持准确。
Fast-ACV的核心思想包括批量注意传播(VAP)和精细到重要(F2I)策略。
VAP可以自动从插值的相关体积中选择准确的相关值，并将这些准确的相关值传播到具有模糊相关线索的周围像素，F2I可以生成一组具有高似然的差异假设和相应的注意权值，从而显著抑制拼接体积中不可能存在的差异，从而减少时间和内存成本。
#### 性能
精度：SOTA on KITTI
速度：由ACVNet的0.2s提升到45ms
![20231204231715](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204231715.png)

#### 结论
提出VAP和F2I策略，显著降低计算和内存成本，同时保持准确。

### 4.HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching

#### Date
2021/06
#### Conference/Publication
2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
#### Author
Vladimir Tankovich Christian H ̈ ane Yinda Zhang Adarsh Kowdle Sean Fanello Sofien Bouaziz  
Google
#### 解决的问题/创新点
解决了构建cost volume 的计算和内存成本高的问题。
与最近许多在全成本体积上运行并依赖3D卷积的神经网络方法相反，我们的方法不明确地构建体积，而是依赖于快速的多分辨率初始化步骤、可微分的2D几何传播和扭曲机制来推断视差假设。为了达到较高的精度，网络不仅从几何上推断差异，而且还推断斜面假设，从而更准确地执行几何翘曲和上采样操作。其计算量仅为最先进方法的一小部分。
#### 性能
精度：ETH3D排名第3，在Middlebury-v3上所有端到端学习排名第1，在流行的KITTI 2012和2015基准中排名第1
速度：0.02s, 50FPS
![20231204233437](https://raw.githubusercontent.com/2c984r83y/picgo_picbed/main/blog_img/20231204233437.png)
#### 结论
基于传统算法的改进，跳过了构建 cost volume 的步骤，显著降低计算和内存成本，提高速度。
但是官方目前开源的 tensorflow model 运行速度是论文中的三倍。
> The released models are using default tensoflow ops which causes them to be 3X slower than the same models that use custom CUDA ops. The custom CUDA ops and a version of the models that use them may be released later. 

### 5.

#### Date

#### Conference/Publication


#### Author


#### 解决的问题/创新点


#### 性能
精度：

速度：

#### 结论

### 6.

#### Date

#### Conference/Publication


#### Author


#### 解决的问题/创新点


#### 性能
精度：

速度：

#### 结论

### 7.

#### Date

#### Conference/Publication


#### Author


#### 解决的问题/创新点


#### 性能
精度：

速度：

#### 结论